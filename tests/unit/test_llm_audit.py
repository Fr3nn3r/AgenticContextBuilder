"""Unit tests for LLM Audit Service."""

import json
import tempfile
from pathlib import Path
from unittest.mock import MagicMock, patch

import pytest

from context_builder.services.llm_audit import (
    AuditedOpenAIClient,
    LLMAuditService,
    LLMCallRecord,
)


@pytest.fixture
def temp_log_dir():
    """Create a temporary directory for audit logs."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield Path(tmpdir)


@pytest.fixture
def audit_service(temp_log_dir):
    """Create an LLMAuditService instance."""
    return LLMAuditService(temp_log_dir)


class TestLLMCallRecord:
    """Tests for LLMCallRecord dataclass."""

    def test_default_call_id_generated(self):
        """Call ID is generated by default."""
        record = LLMCallRecord()
        assert record.call_id.startswith("llm_")
        assert len(record.call_id) > 4

    def test_created_at_generated(self):
        """Created timestamp is generated by default."""
        record = LLMCallRecord()
        assert record.created_at.endswith("Z")

    def test_all_fields_can_be_set(self):
        """All fields can be explicitly set."""
        record = LLMCallRecord(
            call_id="test_call",
            model="gpt-4o",
            temperature=0.5,
            max_tokens=1000,
            messages=[{"role": "user", "content": "Hello"}],
            response_content="World",
            prompt_tokens=10,
            completion_tokens=5,
            total_tokens=15,
            latency_ms=100,
            claim_id="CLM001",
            doc_id="DOC001",
            call_purpose="classification",
        )

        assert record.call_id == "test_call"
        assert record.model == "gpt-4o"
        assert record.temperature == 0.5
        assert record.claim_id == "CLM001"


class TestLLMAuditService:
    """Tests for LLMAuditService."""

    def test_log_call_creates_file(self, audit_service, temp_log_dir):
        """Logging a call creates the log file."""
        record = LLMCallRecord(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Test"}],
        )

        audit_service.log_call(record)

        log_file = temp_log_dir / "llm_calls.jsonl"
        assert log_file.exists()

    def test_log_call_appends_jsonl(self, audit_service, temp_log_dir):
        """Multiple calls are appended as JSONL."""
        for i in range(3):
            record = LLMCallRecord(
                call_id=f"call_{i}",
                model="gpt-4o",
                messages=[{"role": "user", "content": f"Test {i}"}],
            )
            audit_service.log_call(record)

        log_file = temp_log_dir / "llm_calls.jsonl"
        with open(log_file, "r", encoding="utf-8") as f:
            lines = f.readlines()

        assert len(lines) == 3

        for i, line in enumerate(lines):
            data = json.loads(line)
            assert data["call_id"] == f"call_{i}"

    def test_get_by_id(self, audit_service):
        """Can retrieve a call record by ID."""
        record = LLMCallRecord(
            call_id="unique_call_123",
            model="gpt-4o",
            messages=[{"role": "user", "content": "Test"}],
            response_content="Response",
        )
        audit_service.log_call(record)

        retrieved = audit_service.get_by_id("unique_call_123")
        assert retrieved is not None
        assert retrieved.call_id == "unique_call_123"
        assert retrieved.response_content == "Response"

    def test_get_by_id_not_found(self, audit_service):
        """Returns None for non-existent ID."""
        result = audit_service.get_by_id("nonexistent")
        assert result is None

    def test_query_by_decision(self, audit_service):
        """Can query calls by decision_id."""
        # Log calls with different decision IDs
        for i in range(5):
            decision_id = "dec_A" if i % 2 == 0 else "dec_B"
            record = LLMCallRecord(
                call_id=f"call_{i}",
                model="gpt-4o",
                messages=[],
                decision_id=decision_id,
            )
            audit_service.log_call(record)

        results = audit_service.query_by_decision("dec_A")
        assert len(results) == 3  # calls 0, 2, 4


class TestAuditedOpenAIClient:
    """Tests for AuditedOpenAIClient wrapper."""

    @pytest.fixture
    def mock_openai_client(self):
        """Create a mock OpenAI client."""
        client = MagicMock()

        # Mock the response
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = '{"result": "test"}'
        mock_response.choices[0].finish_reason = "stop"
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.completion_tokens = 5
        mock_response.usage.total_tokens = 15

        client.chat.completions.create.return_value = mock_response

        return client

    def test_logs_successful_call(self, mock_openai_client, temp_log_dir):
        """Successful calls are logged."""
        audit_service = LLMAuditService(temp_log_dir)
        audited = AuditedOpenAIClient(mock_openai_client, audit_service)

        response = audited.chat_completions_create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Hello"}],
            temperature=0.5,
            max_tokens=100,
        )

        # Verify the call was made
        mock_openai_client.chat.completions.create.assert_called_once()

        # Verify the call was logged
        log_file = temp_log_dir / "llm_calls.jsonl"
        with open(log_file, "r", encoding="utf-8") as f:
            data = json.loads(f.readline())

        assert data["model"] == "gpt-4o"
        assert data["temperature"] == 0.5
        assert data["response_content"] == '{"result": "test"}'
        assert data["prompt_tokens"] == 10
        assert data["error"] is None

    def test_logs_failed_call(self, mock_openai_client, temp_log_dir):
        """Failed calls are logged with error details."""
        mock_openai_client.chat.completions.create.side_effect = Exception("API Error")

        audit_service = LLMAuditService(temp_log_dir)
        audited = AuditedOpenAIClient(mock_openai_client, audit_service)

        with pytest.raises(Exception, match="API Error"):
            audited.chat_completions_create(
                model="gpt-4o",
                messages=[{"role": "user", "content": "Hello"}],
            )

        # Verify the error was logged
        log_file = temp_log_dir / "llm_calls.jsonl"
        with open(log_file, "r", encoding="utf-8") as f:
            data = json.loads(f.readline())

        assert data["error"] == "API Error"
        assert data["error_type"] == "Exception"

    def test_context_is_logged(self, mock_openai_client, temp_log_dir):
        """Context information is included in logs."""
        audit_service = LLMAuditService(temp_log_dir)
        audited = AuditedOpenAIClient(mock_openai_client, audit_service)

        audited.set_context(
            claim_id="CLM001",
            doc_id="DOC001",
            run_id="RUN001",
            call_purpose="extraction",
        )

        audited.chat_completions_create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Extract"}],
        )

        log_file = temp_log_dir / "llm_calls.jsonl"
        with open(log_file, "r", encoding="utf-8") as f:
            data = json.loads(f.readline())

        assert data["claim_id"] == "CLM001"
        assert data["doc_id"] == "DOC001"
        assert data["run_id"] == "RUN001"
        assert data["call_purpose"] == "extraction"

    def test_retry_tracking(self, mock_openai_client, temp_log_dir):
        """Retry calls are tracked with previous_call_id."""
        # First call fails, second succeeds
        mock_response = MagicMock()
        mock_response.choices = [MagicMock()]
        mock_response.choices[0].message.content = '{"result": "ok"}'
        mock_response.usage = MagicMock()
        mock_response.usage.prompt_tokens = 10
        mock_response.usage.completion_tokens = 5
        mock_response.usage.total_tokens = 15

        mock_openai_client.chat.completions.create.side_effect = [
            Exception("Rate limit"),
            mock_response,
        ]

        audit_service = LLMAuditService(temp_log_dir)
        audited = AuditedOpenAIClient(mock_openai_client, audit_service)

        # First call fails
        with pytest.raises(Exception):
            audited.chat_completions_create(
                model="gpt-4o",
                messages=[{"role": "user", "content": "Test"}],
            )

        # Mark retry
        first_call_id = audited.get_last_call_id()
        audited.mark_retry(first_call_id)

        # Second call succeeds
        audited.chat_completions_create(
            model="gpt-4o",
            messages=[{"role": "user", "content": "Test"}],
        )

        # Check logs
        log_file = temp_log_dir / "llm_calls.jsonl"
        with open(log_file, "r", encoding="utf-8") as f:
            lines = f.readlines()

        assert len(lines) == 2

        first_call = json.loads(lines[0])
        second_call = json.loads(lines[1])

        assert first_call["is_retry"] is False
        assert second_call["is_retry"] is True
        assert second_call["previous_call_id"] == first_call["call_id"]
        assert second_call["attempt_number"] == 2

    def test_unique_call_ids(self, mock_openai_client, temp_log_dir):
        """Each call gets a unique call_id."""
        audit_service = LLMAuditService(temp_log_dir)
        audited = AuditedOpenAIClient(mock_openai_client, audit_service)

        call_ids = set()
        for _ in range(10):
            audited.chat_completions_create(
                model="gpt-4o",
                messages=[{"role": "user", "content": "Test"}],
            )

        log_file = temp_log_dir / "llm_calls.jsonl"
        with open(log_file, "r", encoding="utf-8") as f:
            for line in f:
                data = json.loads(line)
                call_ids.add(data["call_id"])

        assert len(call_ids) == 10  # All unique
